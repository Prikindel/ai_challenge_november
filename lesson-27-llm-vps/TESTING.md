# TESTING — День 27. Локальная LLM на VPS

## 1. Проверка на VPS (локально)
- [ ] `ollama run llama3.2 "hello"` возвращает ответ
- [ ] `ollama list` показывает установленные модели
- [ ] `systemctl status ollama` показывает, что сервис запущен

## 2. Проверка извне (curl через домен)
- [ ] Запрос с basic auth:
```bash
curl https://185.31.165.227/api/generate \
  -u user:pass \
  -H "Content-Type: application/json" \
  -d '{"model":"llama3.2","prompt":"Привет!","stream":false}'
```
- Результат: ___________________________________________
- Ошибки/латентность: __________________________________

- [ ] Запрос с bearer token (если используется):
```bash
curl https://185.31.165.227/api/generate \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"model":"llama3.2","prompt":"Привет!","stream":false}'
```

- [ ] Проверка без авторизации (должна быть ошибка 401):
```bash
curl https://185.31.165.227/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model":"llama3.2","prompt":"Привет!","stream":false}'
```

## 3. Конфигурация клиента
- [ ] Обновлен `config/server.yaml`:
  - baseUrl: `https://185.31.165.227` (HTTPS)
  - model: `llama3.2`
  - auth.type: `basic` или `bearer`
  - auth.user/password или auth.token заполнены
  - timeout: `120000` (2 минуты)

## 4. E2E в чате (клиент)
- [ ] Запуск сервера: `./gradlew run` или через IDE
- [ ] Открытие `http://localhost:8080/client/chat.html`
- [ ] Проверка статуса LLM в UI:
  - Отображается провайдер: `local (vps: llama3.2)`
  - Статус: `✓ VPS (llama3.2)` (зеленый индикатор)
  - При недоступности: `⚠ VPS недоступна` (красный индикатор)

### Сценарии тестирования:

#### 4.1. Короткий вопрос
- [ ] Вопрос: "Привет! Как дела?"
- [ ] Ожидаемый результат: получен ответ от LLM
- [ ] Время ответа: _____________________________
- [ ] Статус в UI: _______________________________

#### 4.2. Длинный ответ
- [ ] Вопрос: "Расскажи подробно о машинном обучении"
- [ ] Ожидаемый результат: получен длинный ответ без таймаута
- [ ] Время ответа: _____________________________
- [ ] Проверка: ответ не обрывается

#### 4.3. Ошибка авторизации
- [ ] Изменить в конфиге: неверный пароль/токен
- [ ] Отправить вопрос в чат
- [ ] Ожидаемый результат: понятное сообщение об ошибке
  - Должно содержать: "Ошибка авторизации" или "неверные учетные данные"
- [ ] Проверка fallback: при ошибке должен быть fallback на OpenRouter (если настроен)

#### 4.4. Проверка HTTPS
- [ ] В логах сервера проверка: запросы идут на HTTPS URL
- [ ] В браузере (DevTools → Network): проверка, что запросы к API идут через HTTPS
- [ ] Проверка сертификата: нет ошибок SSL

## 5. Проверка логов
- [ ] Логи сервера показывают:
  - `Local LLM enabled: provider=ollama, model=llama3.2, baseUrl=https://..., auth=basic`
  - `Using Basic auth for local LLM` (при запросах)
  - При ошибках: понятные сообщения об ошибках авторизации/подключения

## 6. Производительность
- [ ] Время первого запроса (холодный старт): ________________
- [ ] Время последующих запросов: ___________________________
- [ ] Параллельные запросы: _________________________________

## 7. Выводы
- Что работает хорошо: ________________________________
- Что улучшить: ________________________________________
- Рекомендации: ________________________________________

