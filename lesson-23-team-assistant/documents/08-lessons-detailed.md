# Детальная документация по урокам AI Challenge November 2024

## Урок 1: Простой чат-агент

### Технические детали

**Модуль:** `lesson-01-simple-chat-agent`  
**Версия:** 1.0.0  
**Дата создания:** Ноябрь 2024  

### Компоненты

1. **OpenAIClient**
   - HTTP клиент для работы с OpenAI/OpenRouter API
   - Поддержка chat completions
   - Логирование запросов в формате OkHttp

2. **AIRepository**
   - Абстракция работы с AI API
   - Метод `getMessage(userMessage: String): String`
   - Обработка ошибок и пустых ответов

3. **ChatController**
   - HTTP контроллер для чата
   - Endpoint: `POST /chat`
   - Возвращает текстовый ответ от LLM

### Конфигурация

```yaml
ai:
  apiUrl: "https://api.openai.com/v1/chat/completions"
  model: "gpt-3.5-turbo"
  temperature: 0.7
  maxTokens: 2000
```

### Уникальные особенности

- Первый урок с базовой интеграцией LLM
- Простейший пример работы с AI API
- Базовый веб-интерфейс для чата

---

## Урок 2: Структурированные ответы

### Технические детали

**Модуль:** `lesson-02-structured-response`  
**Версия:** 1.0.0  
**Тема:** Энциклопедия животных  

### Компоненты

1. **AnimalEncyclopediaResponse**
   - Sealed class с `@JsonClassDiscriminator("type")`
   - Типы: `success` (данные о животном) и `error` (ошибка валидации)
   - Поля: `name`, `description`, `diet`, `lifespan`, `habitat`

2. **TopicConfig**
   - Конфигурация темы из `config/topic.yaml`
   - Валидация запросов на соответствие теме
   - Код ошибки: `TOPIC_MISMATCH`

3. **PromptBuilder**
   - Автоматическое формирование system prompt из конфигурации темы
   - Инструкции для возврата JSON формата

### Конфигурация

```yaml
ai:
  useJsonFormat: true  # Включить JSON режим

topic:
  name: "Виды животных"
  description: "Энциклопедия о различных видах животных..."
  validationPrompt: |
    Пользователь должен задавать вопросы ТОЛЬКО о животных...
```

### Уникальные особенности

- Использование `response_format: {"type": "json_object"}`
- Полиморфная десериализация через `@JsonClassDiscriminator`
- Валидация темы запроса на стороне LLM

---

## Урок 3: Интерактивный сборщик ТЗ

### Технические детали

**Модуль:** `lesson-03-interactive-tz-agent`  
**Версия:** 1.0.0  

### Компоненты

1. **MessageHistory**
   - Хранение контекста разговора (system prompt + история диалога)
   - Автоматическая обрезка при превышении лимита (50 сообщений)
   - Метод `clear()` для начала нового диалога

2. **TZAgent**
   - Специализированный агент для сбора ТЗ
   - Проверка ответа на маркер `[TZ_READY]`
   - Парсинг JSON ТЗ из ответа

3. **TechnicalSpec**
   - Структура ТЗ: `title`, `description`, `requirements`, `features`, `constraints`, `timeline`, `targetAudience`, `successCriteria`

### Механизм остановки

- System prompt инструктирует модель вернуть `[TZ_READY]` когда информации достаточно
- Агент проверяет наличие маркера в ответе
- Если маркер найден — извлекает и парсит JSON ТЗ

### Уникальные особенности

- Интерактивный диалог с автоматическим определением готовности
- Маркер остановки `[TZ_READY]` для завершения сбора информации
- Структурированный вывод ТЗ в JSON формате

---

## Урок 4: Множественные стратегии рассуждения

### Технические детали

**Модуль:** `lesson-04-multi-reasoning`  
**Версия:** 1.0.0  

### Режимы рассуждения

1. **Direct**
   - Запрос без дополнительных инструкций
   - Быстрый ответ

2. **Step-by-step**
   - Промпт: "Решай пошагово"
   - Подробное объяснение решения

3. **Prompt from other AI**
   - Сначала просим модель придумать лучший промпт
   - Затем выполняем запрос с этим промптом

4. **Expert panel**
   - Симуляция группы экспертов
   - Обсуждение и общий вывод

5. **Comparison**
   - Запрос у LLM для сравнения собранных ответов

### Компоненты

1. **ReasoningAgent**
   - Поддержка всех режимов рассуждения
   - Сбор ответов от разных режимов
   - Сравнение результатов

### Уникальные особенности

- Сравнение разных подходов к решению одной задачи
- Экспериментальный подход к улучшению качества ответов
- LLM-сравнение для анализа различий

---

## Урок 5: Температурные режимы

### Технические детали

**Модуль:** `lesson-05-temperature`  
**Версия:** 1.0.0  

### Компоненты

1. **TemperatureAgent**
   - Выполняет один запрос во всех указанных температурах
   - Собирает: текст ответа, время генерации, статистику токенов
   - JSON-анализ сравнений

2. **TemperatureLessonConfig**
   - Дефолтный вопрос
   - Список температур (например, `[0.0, 0.7, 1.2]`)
   - Temperature для comparison запроса

### Конфигурация

```yaml
topic:
  defaultQuestion: "Логическая задача..."
  temperatures: [0.0, 0.7, 1.2]
  comparisonTemperature: 0.3
```

### Выводы урока

- **0.0** — структурно, чётко, без креатива
- **0.3** — расширенные возможности, немного креатива
- **0.7** — модель мыслит и креативит, оптимальный баланс

### Уникальные особенности

- Интерактивный список температур (добавление, изменение, удаление)
- История запусков в браузере (до 10 последних)
- LLM-сравнение для рекомендаций по использованию температур

---

## Урок 6: Версии моделей

### Технические детали

**Модуль:** `lesson-06-model-versions`  
**Версия:** 1.0.0  
**Провайдер:** Hugging Face Inference Router  

### Каталог моделей

1. **DeepSeek R1** — reasoning-класс, подробные цепочки рассуждений
2. **GPT-OSS 120B** — крупный открытый аналог GPT
3. **Qwen2.5 7B Instruct** — сбалансированный 7B
4. **Gemma 2 9B IT** — модель Google, компактные ответы
5. **Llama 3.2 3B Instruct** — лёгкий baseline
6. **MiniMax M2** — универсальный чат-LLM
7. **Llama 3.2 1B MGSM8K** — ультра-лёгкая модель

### Компоненты

1. **ModelComparisonAgent**
   - Запускает вопрос на выбранных моделях
   - Собирает метаданные (время, токены, стоимость)
   - Формирует сводку различий

2. **ModelComparisonLessonConfig**
   - Каталог моделей из `config/models.yaml`
   - Дефолтный вопрос
   - Дефолтные идентификаторы моделей

### Конфигурация

```yaml
models:
  - id: "deepseek-r1"
    displayName: "DeepSeek R1"
    endpoint: "https://router.huggingface.co/v1/chat/completions"
    pricePer1kTokensUsd: 0.12
```

### Уникальные особенности

- Сравнение моделей по времени, стоимости, качеству
- Интеграция с Hugging Face Inference Router
- Расчёт стоимости на основе `pricePer1kTokensUsd`

---

## Урок 7: Анализ использования токенов

### Технические детали

**Модуль:** `lesson-07-token-usage`  
**Версия:** 1.0.0  
**Token counter:** `jtokkit` (эмуляция `tiktoken`)  

### Сценарии

1. **Короткий запрос**
   - Минимальный промпт
   - Десятки токенов

2. **Длинный запрос**
   - Большой промпт с инструкциями
   - Сотни токенов

3. **Перегруженный запрос**
   - Превышение лимита промпта
   - Статус: `truncated` или `error`

### Компоненты

1. **TokenUsageAgent**
   - Подсчёт токенов через `jtokkit`
   - Проверка лимитов перед запросом
   - Статусы: `success`, `truncated`, `error`

2. **TokenCounter**
   - Обёртка над `jtokkit`
   - Кодировка: `cl100k_base`
   - Подсчёт токенов промпта и ответа

### Конфигурация

```yaml
tokenUsage:
  promptTokenLimit: 1000
  defaultMaxResponseTokens: 512
  tokenEncoding: "cl100k_base"
  scenarios:
    - id: "short-request"
      name: "Короткий запрос"
      defaultPrompt: "..."
```

### Уникальные особенности

- Локальный подсчёт токенов без запроса к API
- Предварительная проверка лимитов
- История запусков с метаданными

---

## Урок 8: Сжатие диалога

### Технические детали

**Модуль:** `lesson-08-dialog-compression`  
**Версия:** 1.0.0  

### Стратегии суммирования

1. **Независимая**
   - Каждое summary создаётся отдельно
   - Все summary включаются в контекст

2. **Кумулятивная**
   - Summary эволюционирует, включая предыдущие
   - В контекст попадает только последнее summary

### Компоненты

1. **DialogCompressionAgent**
   - Автоматическое создание summary каждые N сообщений
   - Хранение истории в виде "сырых" сообщений и summary-узлов
   - Сбор отчёта о контексте

2. **CompressionService**
   - Создание summary через LLM
   - Обрезка истории при превышении лимита
   - Экономия токенов

### Конфигурация

```yaml
compression:
  summaryInterval: 5  # каждые N пользовательских сообщений
  maxSummariesInContext: 3
  rawHistoryLimit: 10
  compressionModel: "gpt-3.5-turbo"
```

### Выводы урока

- Экономия токенов зависит от стратегии суммирования
- Суммирование влияет на скорость ответа
- Качество суммирования критично для качества ответов

### Уникальные особенности

- Две стратегии суммирования для сравнения
- Контрольный прогон сценариев
- Визуализация экономии токенов

---

## Урок 9: Внешняя память

### Технические детали

**Модуль:** `lesson-09-external-memory`  
**Версия:** 1.0.0  

### Типы хранилищ

1. **SQLite**
   - Продакшен-решение
   - Таблица `memory_entries`
   - Индексы для быстрого поиска

2. **JSON**
   - Разработка и отладка
   - Файл `memory.json`
   - Pretty print для читаемости

### Компоненты

1. **MemoryOrchestrator**
   - Координатор работы агентов
   - Управление историей и памятью

2. **MemoryService**
   - Бизнес-логика создания записей
   - Преобразование между форматами

3. **ConversationAgent**
   - Работа с LLM
   - Отправка запросов с историей

4. **MemoryRepository**
   - Абстракция работы с хранилищем
   - Реализации: `SqliteMemoryRepository`, `JsonMemoryRepository`

### Суммаризация и компактация

- Суммаризация каждые `userMessagesPerSummary` сообщений
- Размер сегмента: `userMessagesPerSegment` сообщений
- Компактная история: последняя SUMMARY + сообщения после неё

### Конфигурация

```yaml
memory:
  storageType: "sqlite"  # или "json"
  sqlite:
    databasePath: "data/memory.db"
  summarization:
    enabled: true
    userMessagesPerSummary: 10
    userMessagesPerSegment: 100
```

### Уникальные особенности

- Персистентность между перезапусками
- Автоматическая загрузка истории при старте
- Суммаризация для экономии токенов

---

## Урок 10: Подключение MCP

### Технические детали

**Модуль:** `lesson-10-mcp-connection`  
**Версия:** 1.0.0  
**SDK:** `io.modelcontextprotocol:kotlin-sdk:0.7.7`  

### Компоненты

1. **MCPClient**
   - Обёртка над MCP SDK Client
   - Подключение через stdio транспорт
   - Получение списка инструментов и ресурсов

2. **MCPConnectionAgent**
   - Агент для подключения к MCP
   - Управление соединением
   - Вызов инструментов

3. **MCPRepository**
   - Репозиторий для работы с MCP
   - Кэширование списка инструментов

### Конфигурация

```yaml
mcp:
  servers:
    - name: "weather-server"
      type: "stdio"
      enabled: true
      command: "java"
      args: ["-jar", "mcp-servers/weather-stdio-server.jar"]
```

### Уникальные особенности

- Интеграция с официальным Kotlin SDK для MCP
- Поддержка stdio транспорта
- Визуализация инструментов и ресурсов через UI

---

## Урок 11: Первый инструмент MCP

### Технические детали

**Модуль:** `lesson-11-first-mcp-tool`  
**Версия:** 1.0.0  
**API:** Telegram Bot API  

### MCP Сервер

**Структура:**
- `mcp-server/` — отдельный модуль/процесс
- `MCPServer.kt` — основной класс MCP сервера
- `ToolRegistry.kt` — регистрация инструментов
- `TelegramApiClient.kt` — клиент для Telegram Bot API

### Инструменты

1. **get_bot_info**
   - Получить информацию о Telegram боте (getMe)
   - Параметры: нет

2. **send_message**
   - Отправить сообщение в Telegram чат
   - Параметры: `chatId`, `text`

### Конфигурация

```yaml
mcpServer:
  info:
    name: "telegram-bot-mcp-server"
    version: "1.0.0"
  api:
    baseUrl: "https://api.telegram.org"
    token: "${TELEGRAM_BOT_TOKEN}"
```

### Уникальные особенности

- Создание собственного MCP сервера на Kotlin
- Интеграция с внешним API (Telegram Bot API)
- Регистрация инструментов в MCP сервере

---

## Урок 12: Планировщик + MCP

### Технические детали

**Модуль:** `lesson-12-reminder-mcp`  
**Версия:** 1.0.0  

### MCP Серверы

1. **chat-history-mcp-server**
   - Инструмент: `get_chat_history(startTime, endTime)`
   - Читает из `memory.db` (lesson-09)

2. **telegram-mcp-server**
   - Инструменты: `get_telegram_messages`, `send_telegram_message`
   - Polling для получения сообщений из группы
   - Сохранение в `summary.db`

### Компоненты

1. **LLMWithSummaryAgent**
   - Генерация summary из разных источников
   - Интеграция с MCP инструментами
   - Автоматический вызов нужного инструмента

2. **SchedulerService**
   - Автоматическая генерация summary по расписанию
   - Настраиваемый интервал и период анализа
   - Отправка summary через MCP

### Конфигурация

```yaml
scheduler:
  enabled: true
  intervalMinutes: 15
  periodHours: 24
  activeSource: "telegram"  # или "web_chat"
  delivery:
    telegram:
      enabled: true
      userId: "${TELEGRAM_CHAT_ID}"
```

### Уникальные особенности

- Два независимых MCP сервера для разделения ответственности
- Автоматическая генерация summary по расписанию
- Отправка summary в Telegram через MCP инструмент

---

## Урок 13: Композиция MCP-инструментов

### Технические детали

**Модуль:** `lesson-13-mcp-composition`  
**Версия:** 1.0.0  

### Компоненты

1. **LLMCompositionAgent**
   - Управляет циклом обработки
   - Максимум 10 итераций
   - История диалога сохраняется между итерациями

2. **MCPToolAgent**
   - Вызов инструментов MCP
   - Обработка результатов
   - Интеграция с LLM

### Каскадные вызовы

```
LLM → tool_call → MCPToolAgent → MCP Server → результат → LLM → tool_call → ...
```

### Инструменты

1. **get_chat_history** (из lesson-12)
   - Получить историю переписки за период
   - Параметры: `startTime`, `endTime`

2. **send_telegram_message** (новый)
   - Отправить сообщение пользователю в Telegram
   - Параметры: `userId`, `message`

### Уникальные особенности

- LLM сама решает последовательность вызовов инструментов
- Каскадные вызовы с сохранением истории
- Неблокирующая обработка через корутины

---

## Урок 14: Оркестрация

### Технические детали

**Модуль:** `lesson-14-orchestration`  
**Версия:** 1.0.0  

### MCP Серверы

1. **Data Collection Server**
   - `get_chat_history` — история веб-чата
   - `get_telegram_messages` — сообщения из Telegram
   - `get_file_content` — чтение файлов

2. **Data Processing Server**
   - `analyze_sentiment` — анализ тональности
   - `extract_keywords` — извлечение ключевых слов
   - `calculate_statistics` — расчёт статистики

3. **Reporting Server**
   - `generate_report` — генерация отчёта
   - `save_to_file` — сохранение в файл
   - `send_telegram_message` — отправка в Telegram

### Компоненты

1. **OrchestrationAgent**
   - Универсальный системный промпт
   - LLM сама изучает инструменты из описаний
   - Выбор правильных инструментов из разных серверов

2. **MCPClientManager**
   - Управление несколькими MCP серверами
   - Маршрутизация вызовов к правильному серверу
   - Автоматическое подключение при старте

### Уникальные особенности

- Универсальный системный промпт (не знает о конкретных инструментах)
- LLM сама изучает доступные инструменты
- Длинные флоу взаимодействия (5+ шагов)
- Оркестрация инструментов из разных серверов

---

## Урок 15: Индексация документов

### Технические детали

**Модуль:** `lesson-15-document-indexing`  
**Версия:** 1.0.0  
**Модель эмбеддингов:** `nomic-embed-text` (Ollama)  
**Размерность вектора:** 768  

### Компоненты

1. **TextChunker**
   - Разбивка текста на чанки (500-1000 токенов)
   - Перекрытие между чанками (50-100 токенов)
   - Сохранение структуры Markdown

2. **OllamaClient**
   - Генерация эмбеддингов через Ollama API
   - Модель: `nomic-embed-text`
   - Размерность: 768 чисел

3. **VectorNormalizer**
   - Нормализация к диапазону [0; 1]
   - Или L2 нормализация (опционально)

4. **KnowledgeBaseRepository**
   - Хранение в SQLite
   - Таблицы: `documents`, `document_chunks`
   - Эмбеддинги хранятся как JSON или BLOB

5. **KnowledgeBaseSearchService**
   - Поиск по базе знаний
   - Генерация эмбеддинга для запроса
   - Косинусное сходство для ранжирования

### Конфигурация

```yaml
ollama:
  baseUrl: "http://localhost:11434"
  model: "nomic-embed-text"
  timeout: 30000

knowledgeBase:
  databasePath: "data/knowledge_base.db"

indexing:
  chunkSize: 800
  overlapSize: 100
  documentsPath: "documents"
```

### Уникальные особенности

- Локальная генерация эмбеддингов через Ollama
- Нормализация векторов для консистентности
- Семантический поиск по косинусному сходству
- Полный текст чанков хранится в БД

---

## Урок 16: Первый RAG-запрос

### Технические детали

**Модуль:** `lesson-16-rag-query`  
**Версия:** 1.0.0  
**Провайдер LLM:** OpenRouter  

### Пайплайн RAG-запроса

```
1. Вопрос пользователя
2. Генерация эмбеддинга для вопроса (Ollama)
3. Поиск релевантных чанков (косинусное сходство)
4. Получение текста чанков из БД
5. Формирование промпта с контекстом
6. Запрос к LLM с контекстом (RAG)
7. Запрос к LLM без контекста (обычный)
8. Сравнение ответов
```

### Компоненты

1. **RAGService**
   - Оркестрация RAG-пайплайна
   - Поиск чанков через `KnowledgeBaseSearchService`
   - Формирование промпта через `PromptBuilder`
   - Генерация ответа через `LLMService`

2. **PromptBuilder**
   - Формирование промпта с контекстом из чанков
   - Формат контекста: `[Чанк N] (документ: X, сходство: Y%)\n{content}`
   - Инструкции для LLM по использованию контекста

3. **ComparisonService**
   - Сравнение ответов с RAG и без RAG
   - Генерация обоих ответов
   - Анализ различий

### Конфигурация

```yaml
ai:
  provider: "openrouter"
  apiKey: "${OPENAI_API_KEY}"
  model: "gpt-4o-mini"
  temperature: 0.7
  maxTokens: 2000

rag:
  topK: 3
  minSimilarity: 0.7
  maxContextLength: 2000
```

### API Endpoints

1. **POST /api/rag/query** — RAG-запрос (с контекстом)
2. **POST /api/rag/standard** — обычный запрос (без контекста)
3. **POST /api/rag/compare** — сравнение обоих режимов

### Уникальные особенности

- Сравнение RAG и обычного режима в одном запросе
- Формирование промпта с контекстом из чанков
- UI для визуализации различий между режимами
- Отображение использованных чанков с информацией о сходстве

---

*Документ создан: 16 ноября 2024*  
*Последнее обновление: 16 ноября 2024*  
*Версия документа: 1.0*

