lesson:
  defaultQuestion: >
    Какие ключевые отличия между моделями и в каких сценариях лучше выбрать каждую?
    Сформулируй сравнительный ответ с акцентом на практические рекомендации.
  defaultModelIds:
    - deepseek-ai/DeepSeek-R1
    - Qwen/Qwen2.5-7B-Instruct
    - google/gemma-2-9b-it
models:
  - id: deepseek-ai/DeepSeek-R1
    displayName: DeepSeek R1
    endpoint: https://router.huggingface.co/v1/chat/completions
    huggingFaceUrl: https://huggingface.co/deepseek-ai/DeepSeek-R1
    defaultParams:
      temperature: 0.6
      max_tokens: 700
      top_p: 0.9
  - id: openai/gpt-oss-120b
    displayName: GPT-OSS 120B
    endpoint: https://router.huggingface.co/v1/chat/completions
    huggingFaceUrl: https://huggingface.co/openai/gpt-oss-120b
    defaultParams:
      temperature: 0.65
      max_tokens: 750
      top_p: 0.95
  - id: Qwen/Qwen2.5-7B-Instruct
    displayName: Qwen2.5 7B Instruct
    endpoint: https://router.huggingface.co/v1/chat/completions
    huggingFaceUrl: https://huggingface.co/Qwen/Qwen2.5-7B-Instruct
    defaultParams:
      temperature: 0.6
      max_tokens: 650
      top_p: 0.9
  - id: google/gemma-2-9b-it
    displayName: Gemma 2 9B IT
    endpoint: https://router.huggingface.co/v1/chat/completions
    huggingFaceUrl: https://huggingface.co/google/gemma-2-9b-it
    defaultParams:
      temperature: 0.6
      max_tokens: 650
      top_p: 0.92
  - id: meta-llama/Llama-3.2-3B-Instruct
    displayName: Llama 3.2 3B Instruct
    endpoint: https://router.huggingface.co/v1/chat/completions
    huggingFaceUrl: https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct
    defaultParams:
      temperature: 0.55
      max_tokens: 600
      top_p: 0.9
  - id: MiniMaxAI/MiniMax-M2
    displayName: MiniMax M2
    endpoint: https://router.huggingface.co/v1/chat/completions
    huggingFaceUrl: https://huggingface.co/MiniMaxAI/MiniMax-M2
    defaultParams:
      temperature: 0.6
      max_tokens: 620
      top_p: 0.9

