# Lesson 05: Температурные режимы LLM

## Описание

Урок посвящён исследованию влияния параметра `temperature` на ответы LLM. Мы запускаем одну и ту же аналитическую задачу в нескольких режимах, собираем метаданные (время, токены) и формируем рекомендации, в каких сценариях полезен каждый режим.

## Демонстрация

[Смотреть демо](https://disk.yandex.ru/i/ffLJjbzVD08Owg)

## Цели урока

- Понять, как температура влияет на точность, креативность и разнообразие ответов.
- Реализовать серверного агента, который вызывает LLM с разными температурами и агрегирует результаты.
- Построить UI, позволяющий быстро изменять список температур, запускать эксперименты и сравнивать выводы.
- Сохранять историю запусков прямо в браузере (без перезагрузки).

## Ключевые особенности

- **TemperatureAgent** выполняет один запрос пользователя во всех указанных температурах и собирает:
  - текст ответа,
  - время генерации,
  - статистику токенов,
  - JSON-анализ сравнений.
- **LLM-сравнение**: дополнительный запрос в `AIRepository` строит JSON с рекомендациями, когда использовать каждый режим.
- **Конфигурация урока** (`config/topic.yaml`): хранит дефолтный вопрос и базовые температуры (например, `0.0`, `0.7`, `1.2`).
- **Frontend**:
  - интерактивный список температур (добавление, изменение, удаление),
  - запуск экспериментов и вывод карточек по каждой температуре,
  - секция «Сравнение» и «История запусков»,
  - просмотр LLM-запроса и ответа через модальное окно.
- **Сохранение температур**: после запуска эксперимента UI сохраняет введённый список температур, не возвращаясь к пресетам.

## Технологический стек

- **Backend**: Kotlin, Ktor, kotlinx.serialization.
- **Frontend**: HTML, CSS, Vanilla JS.
- **AI API**: OpenAI / OpenRouter (конфигурация в `config/ai.yaml`).
- **Конфигурация**: YAML файлы (`config/ai.yaml`, `config/topic.yaml`).

## Структура проекта (обновлённая)

```
lesson-05-temperature/
├── config/
│   ├── ai.yaml                 # Настройки модели (модель, temperature по умолчанию и т.д.)
│   ├── ai.yaml.example
│   ├── topic.yaml              # Дефолтный вопрос, температуры и comparison temperature
│   └── topic.yaml.example
├── server/
│   └── src/main/kotlin/com/prike/
│       ├── Config.kt           # Загрузка .env + YAML, выбор директории урока
│       ├── config/
│       │   └── TemperatureLessonConfig.kt
│       ├── data/
│       │   ├── client/OpenAIClient.kt   # Поддержка динамической температуры и JSON mode по запросу
│       │   └── repository/AIRepositoryImpl.kt
│       ├── domain/
│       │   ├── agent/TemperatureAgent.kt
│       │   ├── entity/LLMCompletionResult.kt
│       │   └── repository/AIRepository.kt
│       └── presentation/
│           ├── controller/TemperatureController.kt
│           └── dto/TemperatureDtos.kt
└── client/
    ├── index.html              # UI экспериментов
    ├── style.css               # Неоновая тема + карты температур
    └── app.js                  # Логика фронтенда, управление состоянием и историей
```

## Быстрый старт

1. **Java**: установите JDK 17+.
2. **API-ключ**: запишите `OPENAI_API_KEY` в корневой `.env`.
3. **Конфигурация**:
   - `config/ai.yaml` — настройки модели, `useJsonFormat: false` (ответы текстом), system prompt можно кастомизировать.
   - `config/topic.yaml` — дефолтный вопрос и список температур.
4. **Запуск сервера**:
   ```bash
   cd lesson-05-temperature/server
   ./gradlew run   # либо через IDE
   ```
5. **Фронтенд**: откройте `lesson-05-temperature/client/index.html` в браузере.

## API

### `GET /temperature`
Возвращает дефолтный вопрос и массив температур.

### `POST /temperature`
```json
{
  "question": "опциональный текст вопроса",
  "temperatures": [0.0, 0.7, 1.2]
}
```

Ответ:
```json
{
  "defaultQuestion": "...",
  "defaultTemperatures": [0.0, 0.7, 1.2],
  "question": "...",
  "results": [
    {
      "mode": "Температура 0",
      "temperature": 0.0,
      "answer": "...",
      "meta": {
        "durationMs": 1234,
        "promptTokens": 100,
        "completionTokens": 120,
        "totalTokens": 220,
        "requestJson": "{...}",
        "responseJson": "{...}"
      }
    }
  ],
  "comparison": {
    "summary": "Краткое резюме различий",
    "perTemperature": [
      {
        "temperature": 0.0,
        "mode": "Температура 0",
        "accuracy": "...",
        "creativity": "...",
        "diversity": "...",
        "recommendation": "..."
      }
    ]
  }
}
```

## Фронтенд: основные сценарии

- **Редактирование температур**: список можно менять до запуска и он сохраняется после получения ответов.
- **История запусков**: отображает до 10 последних экспериментов; можно раскрыть карточку для повторного просмотра.
- **Сравнительный анализ**: секция «Сравнение» показывает summary и метрики из JSON-анализа.
- **Метаданные**: в карточке каждой температуры есть кнопка «Посмотреть JSON», открывающая запрос и ответ LLM.

## Рекомендации по вопросам

- Логические задачи (например, про распределение подарков) — проверка точности и пошаговых объяснений.
- Вывод объяснений или сравнений — позволяет увидеть баланс фактов и креативности.
- Генеративные запросы (идеи, сценарии, варианты формулировок) — показывают, где высокая температура полезна.

## Полезные заметки

- Температуры ограничены диапазоном `0.0–2.0`, значения округляются до `0.01`.
- Для анализа сравнений используется отдельный JSON-запрос с `AIResponseFormat.JSON_OBJECT`; при ошибке возвращаем дефолтные рекомендации.
- Если сервер запускается не из папки урока, `Config` и `AppModule` найдут директорию `lesson-05-temperature` автоматически.

Урок помогает на практике увидеть, как на одних и тех же данных выбор температуры меняет характер ответа LLM, и какие режимы лучше подходят для точности, а какие — для генерации идей.
