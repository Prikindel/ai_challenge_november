# Lesson 07: Анализ использования токенов

## Описание

Урок демонстрирует, как собирать и анализировать метрики по токенам для разных типов запросов к LLM. Мы запускаем три сценария (короткий, длинный и специально перегруженный), считаем токены промпта и ответа, фиксируем статусы (`success`, `truncated`, `error`) и сохраняем историю запусков. Пользователь может отредактировать тексты подсказок прямо в интерфейсе и мгновенно увидеть, как это повлияло на расход токенов.

## Демонстрация

[Тыкай сюда](https://disk.yandex.ru/i/aHaJLy7xRbwU6A)

## Цели

- Понять, как устроен лимит контекста модели и как отслеживать превышения.
- Реализовать агент, который комбинирует данные от провайдера и локальный подсчёт токенов.
- Добавить API для запуска преднастроенных сценариев и возврата истории.
- Построить UI, который показывает результаты в виде карточек и хранит последовательность запусков.

## Выводы

- **Короткий сценарий** стабильно укладывается в десятки токенов и показывает минимальную стоимость обращения к модели.
- **Длинный сценарий** наглядно демонстрирует рост расходов и времени, когда контекст нагружается большим количеством инструкций.
- **Превышение лимита** срабатывает без запроса к API: агент заранее считает токены и возвращает `error`, экономя ресурсы. При снятии ограничения можно наблюдать, как модель продолжает отвечать вплоть до физических лимитов (например, ~2200 токенов промпта + ~1900 токенов ответа ≈ 3500 токенов суммарно), но качество быстро деградирует.
- **Приближение к порогу** контекста приводит к «галлюцинациям»: модель выдаёт шумный текст, однако ответ всё равно сформируется — важно дождаться завершения и анализировать не только исходный промпт, но и сгенерированный ответ (какие рассуждения или идеи в нём появились).
- **Параметры генерации** (температура, выбор модели) существенно влияют на стабильность: строгие модели с низкой температурой галлюцинируют реже, а креативные конфигурации требуют дополнительного контроля.


## Архитектура и стек

- **Backend:** Kotlin, Ktor, Clean Architecture (`domain` / `data` / `presentation`).
- **Token counter:** библиотека `jtokkit` (локальная эмуляция `tiktoken`).
- **Frontend:** HTML, CSS (градиентная тёмная тема), Vanilla JS (модули не используются).
- **Конфигурация:** YAML (`config/ai.yaml`, `config/token-usage.yaml`) + `.env`.
- **Интеграция:** OpenAI API (chat completions) c использованием `OPENAI_API_KEY`.

## Структура

```
lesson-07-token-usage/
├── config/
│   ├── ai.yaml               # Базовый конфиг доступа к API (URL, модель, temperature, timeout)
│   └── token-usage.yaml      # Настройки лимитов, кодировки и сценариев
├── example.env               # Шаблон окружения (скопируйте в .env в корне урока)
├── server/
│   ├── build.gradle.kts
│   ├── settings.gradle.kts
│   ├── gradle/
│   │   └── wrapper/gradle-wrapper.properties
│   └── src/main/kotlin/com/prike/
│       ├── Config.kt                        # Загрузка .env и YAML текущего урока
│       ├── config/
│       │   ├── AIConfig.kt
│       │   └── TokenUsageLessonConfig.kt
│       ├── data/
│       │   ├── client/OpenAIClient.kt       # Клиент OpenAI Chat Completions
│       │   └── repository/AIRepositoryImpl.kt
│       ├── domain/
│       │   ├── agent/TokenUsageAgent.kt     # Подсчёт токенов и сценарии
│       │   ├── service/TokenCounter.kt      # Обёртка над jtokkit
│       │   └── repository/AIRepository.kt
│       ├── presentation/
│       │   ├── controller/TokenUsageController.kt
│       │   └── dto/TokenUsageDtos.kt
│       └── Main.kt                           # Инициализация Ktor и DI
└── client/
    ├── index.html            # UI сценариев, результатов и истории
    ├── style.css             # Стилевые карточки, статусы, лоадеры
    └── app.js                # Логика работы с API, рендер, обработка ошибок
```

## Конфигурация

### `.env`

Создайте файл `lesson-07-token-usage/.env` на основе `example.env`:

```
OPENAI_API_KEY=sk-...    # можно использовать OpenRouter-токен, указав его в этой переменной
SERVER_HOST=0.0.0.0      # опционально
SERVER_PORT=8080         # опционально
```

> Переменная `OPENAI_API_KEY` обязательна. Для OpenRouter используйте fine-grained токен и задайте его в этом поле (совместимо с текущим клиентом).

### `config/ai.yaml`

- `apiUrl` указывает на `https://openrouter.ai/api/v1/chat/completions`.
- `model` — `meta-llama/llama-3.1-8b-instruct` (как в уроке 05).
- `maxTokens` задаёт базовый максимум для ответа, но каждый сценарий может его переопределить.
- `systemPrompt` и остальные параметры передаются во все сценарии.

### `config/token-usage.yaml`

- `promptTokenLimit` — ожидаемый лимит токенов промпта (используется для статусов и контроля ошибок).
- `defaultMaxResponseTokens` — fallback-значение, если в `ai.yaml` или сценарии нет `maxResponseTokens`.
- `tokenEncoding` — кодировка для `jtokkit` (по умолчанию `cl100k_base`).
- `scenarios` — массив базовых сценариев с `id`, `name`, текстом подсказки и дополнительными параметрами (`temperature`, `maxResponseTokens`).

## API

### `GET /api/token-usage/scenarios`

Возвращает шаблоны сценариев и параметры лимитов.

```json
{
  "scenarios": [
    {
      "scenarioId": "short-request",
      "scenarioName": "Короткий запрос",
      "defaultPrompt": "...",
      "description": "..."
    }
  ],
  "promptTokenLimit": 1000,
  "defaultMaxResponseTokens": 512,
  "tokenEncoding": "cl100k_base"
}
```

### `POST /api/token-usage/analyze`

Запускает анализ: выполняет запросы к модели и возвращает текущий запуск и историю.

Запрос:

```json
{
  "scenarios": [
    {
      "scenarioId": "short-request",
      "promptText": "кастомный текст (опционально)"
    }
  ]
}
```

Ответ:

```json
{
  "currentRun": {
    "runId": "...",
    "startedAt": "2025-11-12T11:25:03.152Z",
    "finishedAt": "2025-11-12T11:25:08.204Z",
    "results": [
      {
        "scenarioId": "short-request",
        "scenarioName": "Короткий запрос",
        "promptText": "...",
        "responseText": "...",
        "promptTokens": 88,
        "responseTokens": 120,
        "totalTokens": 208,
        "durationMs": 4978,
        "status": "success",
        "errorMessage": null
      }
    ]
  },
  "history": [
    {
      "...": "..."
    }
  ]
}
```

- Статусы: `success`, `truncated`, `error`.
- При ошибке `status=error` и `errorMessage` содержит текст исключения или описание превышения лимита.

### `GET /api/token-usage/history`

Возвращает сохранённую историю запусков (максимум `historyLimit` элементов).

## Запуск

1. **Скопируйте конфиг окружения:**
   ```bash
   cp lesson-07-token-usage/example.env lesson-07-token-usage/.env
   ```
   Заполните `OPENAI_API_KEY`.

2. **Установите Gradle (если не установлен)** или добавьте Gradle Wrapper.
   ```bash
   brew install gradle        # macOS
   # либо используйте установленный gradle из PATH
   ```

3. **Соберите и запустите сервер:**
   ```bash
   cd lesson-07-token-usage/server
   gradle build   # или ./gradlew build, если добавите wrapper
   gradle run
   ```

4. **Откройте клиент:**
   - вариант A: запустить сервер Ktor и открыть `http://localhost:8080/` (статические файлы отдаёт `ClientController`);
   - вариант B: открыть `lesson-07-token-usage/client/index.html` напрямую в браузере (при этом запросы будут идти на `http://localhost:8080`).

5. **Запустите анализ в UI:**
   - при необходимости отредактируйте подсказки;
   - нажмите «Запустить анализ» и дождитесь карточек результатов и истории.

## Проверка

- `GET /api/token-usage/scenarios` возвращает список шаблонов и лимиты.
- `POST /api/token-usage/analyze` выдаёт массив результатов — каждый содержит токены, статус и ответ/ошибку.
- Сценарий `overflow-request` должен либо вернуть `status=truncated`, либо `status=error` с сообщением о лимите.
- История хранит последние `historyLimit` запусков и возвращается через `/api/token-usage/history`.

## Ограничения

- Необходимо указать актуальный `OPENAI_API_KEY`; без него сервер не стартует.
- За лимит промпта отвечает локальная логика: при превышении сразу возвращается `status=error`.
- Если провайдер не вернул usage-метрики, агент оценивает токены через `jtokkit` (погрешность возможна).
- Для запуска Gradle команд требуется либо локально установленный Gradle, либо добавление `gradle-wrapper.jar`.

Урок формирует практику контроля токенов в реальном времени: UI показывает, как изменения запросов влияют на расход контекста, а бэкенд сохраняет историю и статус каждого сценария. Это база для мониторинга и алертинга по лимитам LLM в продуктивных системах.
