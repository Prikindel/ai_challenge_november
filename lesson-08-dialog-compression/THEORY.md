# Теория: компрессия диалогов

## 1. Почему исторический контекст дорогой
- **Линейный рост промпта.** Каждый новый запрос к LLM включает все предыдущие сообщения. Длина промпта растёт линейно, повышая стоимость и задержку ответа.
- **Жёсткие лимиты контекста.** Модели имеют ограничение на количество токенов (prompt + completion). Переполнение ведёт к усечению или ошибкам.
- **Влияние на качество.** В длинном контексте полезная информация размывается шумом. Модель может «забыть» ранние детали или сгенерировать неточный ответ.
- **Стоимость и масштабирование.** При большом количестве диалогов поддержание полной истории резко увеличивает расходы на API.

## 2. Подходы к компрессии истории

### 2.1 Rolling Summary
- **Суть.** Периодически объединяем последние `N` сообщений в компактный конспект (summary) и заменяем ими исходные реплики.
- **Когда полезно.** Диалоги с линейным развитием, где важно сохранить основные договорённости, но не каждую реплику.
- **Как настроить.** Выбрать интервал `summaryInterval`, формат ответа (например, поля `summary`, `facts`, `openQuestions`) и модель для сжатия.

### 2.2 Retrieval-based
- **Суть.** Важные факты, решения, данные сохраняем отдельно (в «памяти», векторном хранилище, базе). Перед новым запросом выбираем наиболее релевантные фрагменты.
- **Плюсы.** Точное восстановление деталей; масштабируется для больших массивов данных.
- **Минусы.** Требует инфраструктуры для индексации и поиска; важно настроить релевантность.

### 2.3 Hybrid (Rolling Summary + Retrieval)
- **Комбинация.** Summary обеспечивает общий обзор, а retrieval возвращает конкретные факты по необходимости.
- **Сценарии.** Долгие проекты, где важны как общие договорённости, так и детали (ссылки, цифры, имена).

## 3. Компромиссы и риски
- **Потеря нюансов.** При агрессивном сжатии исчезают контекстные подсказки, тон, формулировки клиента.
- **Искажения.** Модель может «додумать» факты. Нужно задавать строгий формат summary и проверять его содержимое.
- **Частота обновления.** Если summary обновляется слишком редко, экономия падает. Слишком часто — растёт нагрузка на модель и риск потерь.
- **Согласованность.** Изменённый summary должен соответствовать фактической истории; важно хранить ссылки на исходные сообщения.

## 4. Метрики оценки
- **Prompt tokens.** Фактическая длина запроса. Главный индикатор экономии.
- **Hypothetical prompt tokens.** Сколько токенов потребовалось бы без сжатия. Позволяет измерить выгоду.
- **Completion tokens.** Может расти при потере контекста (модель перепроверяет факты).
- **Total tokens и стоимость.** Общие затраты на запрос.
- **Логическая связность и точность.** Проверяем, сохраняется ли смысл ответов.
- **Субъективное восприятие пользователя.** Насколько итоговые ответы понятны и удовлетворяют ожиданиям.

## 5. Практические рекомендации
- Используйте явный формат summary (ключевые факты, открытые вопросы, решения). Это снижает риск искажений.
- Храните связи между summary и исходными сообщениями — пригодится для отладки и восстановления контекста.
- Ограничивайте количество summary в промпте (`maxSummariesInContext`), чтобы не превратить их в ещё одну «длинную историю».
- Следите за `rawHistoryLimit`: он должен покрывать «горячие» сообщения, которые ещё не вошли в summary.
- Для контрольных прогонов фиксируйте расход токенов в двух режимах — разница показывает эффект компрессии.
- Если модель не возвращает usage-метрики, рассчитывайте токены локально (например, через библиотеку tiktoken/jtokkit) и сохраняйте результаты.
- Регулярно пересматривайте интервал свёртки: если экономия мала, уменьшите `summaryInterval`; если теряются детали — увеличьте.

## 6. Паттерн работы агента
1. Получаем новое сообщение пользователя и добавляем его в «сырую» историю.
2. Собираем контекст: summary-узлы (с учётом лимита) + последние несжатые сообщения.
3. Запрашиваем LLM, фиксируем usage и гипотетические токены.
4. Добавляем ответ ассистента к истории.
5. Если число несжатых сообщений достигло `summaryInterval`, строим summary, сохраняем его и архивируем исходные реплики.
6. Повторяем шаги для следующего сообщения.

Такой цикл позволяет поддерживать диалог, снижая затраты и сохраняя ключевые знания.
