## Задача
Оптимизировать локальную LLM под конкретную задачу, изменяя параметры модели (temperature, maxTokens, contextWindow и др.) и настраивая prompt-шаблоны для повышения точности ответов.

Полный промт в PROMT.md

## Требования
- Поддержка изменения параметров LLM (temperature, maxTokens, topP, topK, repeatPenalty, contextWindow, seed)
- Система prompt-шаблонов для разных задач (default, code_assistant, qa_assistant и др.)
- Интеграция параметров и шаблонов в LLMService
- UI для выбора параметров и шаблонов
- Система тестирования для сравнения разных конфигураций
- Документация найденных оптимальных параметров

## План (коммиты) - автоматизируемая часть
1) Выбор базового урока lesson-27-llm-vps и копирование структуры
2) Модель параметров: создать LLMParameters, обновить конфигурацию (config/server.yaml)
3) Обновление LocalLLMClient: добавить передачу параметров в запросы к Ollama API
4) Система шаблонов: создать PromptTemplate, PromptTemplateRepository, PromptTemplateService
5) Интеграция шаблонов в LLMService: использовать шаблоны при генерации ответов
6) UI для параметров: форма выбора параметров и шаблонов, сохранение в localStorage
7) Система тестирования: LLMTest, TestResult, LLMTestService, API endpoint для запуска тестов
8) UI для тестов: интерфейс для запуска тестов и сравнения результатов
9) Документация: OPTIMIZATION_RESULTS.md, PARAMETERS_GUIDE.md, обновить README.md

## Ключевые параметры
- temperature: 0.0-2.0 (креативность, по умолчанию 0.7)
- maxTokens: максимальная длина ответа (по умолчанию 2048)
- contextWindow: размер контекста (по умолчанию 4096)
- topP: 0.0-1.0 (nucleus sampling, по умолчанию 0.9)
- topK: top-k sampling (по умолчанию 40)
- repeatPenalty: штраф за повторения (по умолчанию 1.1)

## Примеры шаблонов
- default: стандартный промпт "{user_message}"
- code_assistant: для программирования с системным промптом
- qa_assistant: для точных ответов с контекстом "{context}"

## Тестирование
- Предустановленные тесты: code_questions, general_qa
- Сравнение результатов: время ответа, качество ответов
- Рекомендации по параметрам для разных задач

## Примечания
- Использовать базовый урок с локальной LLM (lesson-25 или lesson-27)
- Параметры передаются в options для Ollama API
- Шаблоны поддерживают плейсхолдеры: {user_message}, {context}
- Фиксировать найденные оптимальные параметры в документации

