## Задача
Подключить чат-приложение (урок 25/26) к локальной LLM на VPS через HTTPS с авторизацией.

## ⚠️ ВАЖНО
**Этот промпт только для автоматизируемой части (код приложения).**

**Ручные шаги на VPS** (установка Ollama, nginx, firewall) описаны в `VPS_SETUP.md` и должны быть выполнены вручную перед началом работы агента.

## Предполагается
- VPS уже настроен (Ollama установлен, модель скачана)
- Реверс-прокси настроен (HTTPS + auth)
- Доступ к VPS проверен через curl

## Требования к коду
- Обновить конфигурацию клиента для подключения к VPS
- Добавить поддержку HTTPS и авторизации (basic/bearer) в `LocalLLMClient`
- Обновить UI для отображения статуса VPS
- Протестировать подключение

## План (коммиты) - только автоматизируемая часть
1) Клиент: обновить конфиг (`baseUrl`, auth, timeout), поддержка HTTPS/basic/bearer в `LocalLLMClient`.
2) UI: показать провайдера `local (vps: llama3.2)`, включить по умолчанию.
3) Тесты: короткий запрос, длинный ответ/таймаут, ошибка auth → `TESTING.md`.
4) Документация: обновить `README.md` с примерами конфигурации.

## Ключевые команды
```bash
curl -fsSL https://ollama.com/install.sh | sh
systemctl enable --now ollama
ollama pull llama3.2
ollama run llama3.2 "hello"
```

Проверка извне:
```bash
curl https://185.31.165.227/api/generate \
  -u user:pass \
  -H "Content-Type: application/json" \
  -d '{"model":"llama3.2","prompt":"Привет!","stream":false}'
```

## Примечания
- HTTPS обязателен; auth обязателен; 11434 не светить наружу.
- При необходимости OpenAI-совместимого API — рассмотреть llama.cpp server `--api`.

