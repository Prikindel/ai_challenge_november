# Инструкции по установке локальной LLM

**Важно:** Заполните этот файл после установки локальной LLM.

## Выбранный провайдер

[Укажите выбранный провайдер: Ollama, LM Studio, LLaMA.cpp или другой]

## Процесс установки

### Шаг 1: Установка

[Опишите процесс установки]

### Шаг 2: Запуск

[Опишите как запустить локальную LLM]

### Шаг 3: Установка модели

[Опишите какую модель установили и как]

## Проверка работы

### Через CLI

```bash
[Команда для проверки через CLI]
```

**Результат:**
```
[Пример ответа]
```

### Через API

```bash
[Команда curl для проверки через API]
```

**Результат:**
```json
[Пример JSON ответа]
```

## Конфигурация

**URL API:** [например, http://localhost:11434]

**Порт:** [например, 11434]

**Модель:** [например, llama3.2]

**API Path:** [например, /api/generate]

## Примеры запросов

### Пример 1: Простой вопрос

```bash
[Команда]
```

**Ответ:**
```
[Ответ]
```

### Пример 2: [Название примера]

```bash
[Команда]
```

**Ответ:**
```
[Ответ]
```

## Проблемы и решения

[Если были проблемы при установке, опишите их и решения]

## Примечания

[Любые дополнительные заметки]

